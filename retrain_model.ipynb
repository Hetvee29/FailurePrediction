{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe1df67-38af-4232-91d8-48eb703eea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MACHINE FAILURE PREDICTION - MODEL RETRAINING PIPELINE\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d58d74-e479-4c95-8c74-4b4a8452735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.type_encoder = OrdinalEncoder(categories=[['L', 'M', 'H']])\n",
    "        self.failure_encoder = LabelEncoder()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.scale_cols = ['Rotational_speed_rpm', 'Torque_Nm', 'Tool_wear_min',\n",
    "                           'Air_temperature_C', 'Process_temperature_C']\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.type_encoder.fit(X[['Type']])\n",
    "        self.failure_encoder.fit(X['Type_of_failure'])\n",
    "        self.scale_cols = [col for col in self.scale_cols if col in X.columns]\n",
    "        self.scaler.fit(X[self.scale_cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['Type'] = self.type_encoder.transform(X[['Type']])\n",
    "        if 'Type_of_failure' in X.columns:\n",
    "            X['Type_of_failure'] = self.failure_encoder.transform(X['Type_of_failure'])\n",
    "        X[self.scale_cols] = self.scaler.transform(X[self.scale_cols])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eda6170-ce80-446d-9890-78d1465c49ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Retraining Pipeline...\n",
      "Loading data and preprocessing pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MLOps_Project\\FailurePrediction\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator OrdinalEncoder from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\MLOps_Project\\FailurePrediction\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\MLOps_Project\\FailurePrediction\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Retraining Pipeline...\")\n",
    "print(\"Loading data and preprocessing pipeline...\")\n",
    "\n",
    "# Load processed data and pipeline\n",
    "df = pd.read_csv('data_preprocessed.csv')\n",
    "preprocessor = joblib.load('preprocessing_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e256b1f-3d56-4613-bab3-d17807e12d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for retraining...\n",
      "Data split complete → Train: (8000, 6), Test: (2000, 6)\n",
      "Applying SMOTE for balancing...\n",
      "SMOTE applied → Balanced Train Shape: (15444, 6)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: DATA PREPARATION\n",
    "# ============================================================\n",
    "print(\"Preparing data for retraining...\")\n",
    "\n",
    "# Binary classification preparation\n",
    "binary_df = df.drop(['Type_of_failure'], axis=1)\n",
    "X = binary_df.drop(columns=['Machine_failure'])\n",
    "y = binary_df['Machine_failure']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Data split complete → Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# SMOTE balancing\n",
    "print(\"Applying SMOTE for balancing...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
    "print(f\"SMOTE applied → Balanced Train Shape: {X_train_bal.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ecb506f-31f5-4f35-b5c3-5b9d2ab60c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training base models...\n",
      "Logistic Regression: Mean F1 = 0.829 ± 0.008\n",
      "Random Forest: Mean F1 = 0.963 ± 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MLOps_Project\\FailurePrediction\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [03:51:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\MLOps_Project\\FailurePrediction\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [03:51:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\MLOps_Project\\FailurePrediction\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [03:51:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\MLOps_Project\\FailurePrediction\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [03:51:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "d:\\MLOps_Project\\FailurePrediction\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [03:51:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: Mean F1 = 0.964 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: DEFINE MODELS\n",
    "# ============================================================\n",
    "print(\"\\nTraining base models...\")\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000, solver='saga', random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(max_depth=10, n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(max_depth=4, learning_rate=0.1, n_estimators=200,\n",
    "                             subsample=0.8, colsample_bytree=0.8,\n",
    "                             random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Cross-validation results\n",
    "for name, model in classifiers.items():\n",
    "    scores = cross_val_score(model, X_train_bal, y_train_bal, cv=5, scoring='f1')\n",
    "    print(f\"{name}: Mean F1 = {np.mean(scores):.3f} ± {np.std(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc6b98e-7aea-4962-97c3-881aebf28bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating base models on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MLOps_Project\\FailurePrediction\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [03:51:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Model Results:\n",
      "                    Accuracy Precision    Recall  F1-Score   ROC-AUC  \\\n",
      "Logistic Regression   0.8245  0.130102  0.836066  0.225166    0.9018   \n",
      "Random Forest          0.956  0.395349  0.836066  0.536842  0.954785   \n",
      "XGBoost                0.954  0.377953  0.786885  0.510638  0.967331   \n",
      "\n",
      "                                                                 Model  \n",
      "Logistic Regression  LogisticRegression(max_iter=5000, random_state...  \n",
      "Random Forest        (DecisionTreeClassifier(max_depth=10, max_feat...  \n",
      "XGBoost              XGBClassifier(base_score=None, booster=None, c...  \n"
     ]
    }
   ],
   "source": [
    "# STEP 3: TRAIN & EVALUATE BASE MODELS\n",
    "# ============================================================\n",
    "print(\"\\nEvaluating base models on test data...\")\n",
    "results = {}\n",
    "for name, model in classifiers.items():\n",
    "    model.fit(X_train_bal, y_train_bal)\n",
    "    preds = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"Precision\": precision_score(y_test, preds, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, preds, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_test, preds, zero_division=0),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, probs) if probs is not None else None,\n",
    "        \"Model\": model\n",
    "    }\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(\"\\nBase Model Results:\")\n",
    "print(df_results.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c293bb82-95d5-4376-baab-4fd06c31a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Random Forest hyperparameter tuning...\n",
      "Best RF Params: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: RANDOM FOREST HYPERPARAMETER TUNING\n",
    "# ============================================================\n",
    "print(\"\\nPerforming Random Forest hyperparameter tuning...\")\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'max_depth': [None, 6, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_grid = RandomizedSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), rf_params,\n",
    "                             n_iter=10, cv=4, scoring='f1', random_state=42, n_jobs=-1)\n",
    "rf_grid.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "print(f\"Best RF Params: {rf_grid.best_params_}\")\n",
    "best_rf = rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21629c1e-ee4d-463d-8560-21dca76b33f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting the best performing model...\n",
      "                      Accuracy Precision    Recall  F1-Score   ROC-AUC  \\\n",
      "Logistic Regression     0.8245  0.130102  0.836066  0.225166    0.9018   \n",
      "Random Forest            0.956  0.395349  0.836066  0.536842  0.954785   \n",
      "XGBoost                  0.954  0.377953  0.786885  0.510638  0.967331   \n",
      "Random Forest (Tuned)   0.9655      0.46  0.754098  0.571429  0.964127   \n",
      "\n",
      "                                                                   Model  \n",
      "Logistic Regression    LogisticRegression(max_iter=5000, random_state...  \n",
      "Random Forest          (DecisionTreeClassifier(max_depth=10, max_feat...  \n",
      "XGBoost                XGBClassifier(base_score=None, booster=None, c...  \n",
      "Random Forest (Tuned)  (DecisionTreeClassifier(max_features='sqrt', m...  \n",
      "\n",
      "Best Model Selected: Random Forest (Tuned)\n",
      "Model saved as 'best_model_binary_final_20251103_035243.pkl'\n",
      "Retraining completed at: 2025-11-03 03:52:43.356255\n",
      "Retraining pipeline executed successfully.\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: ADD TUNED RF AND PICK BEST MODEL\n",
    "# ============================================================\n",
    "print(\"\\nSelecting the best performing model...\")\n",
    "\n",
    "# Add tuned model\n",
    "results[\"Random Forest (Tuned)\"] = {\n",
    "    \"Accuracy\": accuracy_score(y_test, best_rf.predict(X_test)),\n",
    "    \"Precision\": precision_score(y_test, best_rf.predict(X_test), zero_division=0),\n",
    "    \"Recall\": recall_score(y_test, best_rf.predict(X_test), zero_division=0),\n",
    "    \"F1-Score\": f1_score(y_test, best_rf.predict(X_test), zero_division=0),\n",
    "    \"ROC-AUC\": roc_auc_score(y_test, best_rf.predict_proba(X_test)[:, 1]),\n",
    "    \"Model\": best_rf\n",
    "}\n",
    "\n",
    "# Rebuild results table\n",
    "df_final = pd.DataFrame(results).T\n",
    "print(df_final.round(3))\n",
    "\n",
    "# Select best based on F1-score\n",
    "best_model_name = df_final['F1-Score'].idxmax()\n",
    "best_model = df_final.loc[best_model_name, 'Model']\n",
    "print(f\"\\nBest Model Selected: {best_model_name}\")\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"best_model_binary_final_{timestamp}.pkl\"\n",
    "\n",
    "# Save the best model and pipeline\n",
    "joblib.dump(best_model, filename)\n",
    "joblib.dump(preprocessor, 'preprocessing_pipeline.pkl')\n",
    "\n",
    "print(f\"Model saved as '{filename}'\")\n",
    "print(\"Retraining completed at:\", datetime.datetime.now())\n",
    "print(\"Retraining pipeline executed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a1210-057f-474c-91ec-791b28bd8c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
